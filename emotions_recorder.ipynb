{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "from utils.wide_resnet import WideResNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If false, loads video file source\n",
    "USE_WEBCAM = True \n",
    "filename = './test_recordings/_.mp4'\n",
    "filename_result = './result_emotions/_.csv'\n",
    "\n",
    "# Parameters for loading data and images\n",
    "emotion_model_path = './models/emotion_model.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "\n",
    "# Hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "# Loading emotions model\n",
    "face_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3] # getting input model shapes for inference\n",
    "emotion_window = [] # starting lists for calculating modes\n",
    "\n",
    "# Loading age-gender model\n",
    "img_size = 64\n",
    "model = WideResNet(img_size, depth=16, k=8)()\n",
    "model.load_weights(os.path.join(\"models\", \"weights.18-4.06.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To save \n",
    "res_df = pd.DataFrame(columns=['time', 'reaction', 'reaction_tense', 'gender', 'age'])\n",
    "\n",
    "# Select video or webcam feed\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "cap = None\n",
    "if (USE_WEBCAM == True):\n",
    "    cap = cv2.VideoCapture(0) # Webcam source\n",
    "else:\n",
    "    cap = cv2.VideoCapture(filename) # Video file source\n",
    "\n",
    "# Press 'q' to quit\n",
    "old_gender = ''\n",
    "old_age = 0\n",
    "i = 0\n",
    "start_time = time.time()\n",
    "while cap.isOpened():\n",
    "    ret, bgr_image = cap.read()\n",
    "\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    num_rows, num_cols = gray_image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 90, 1)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,\n",
    "        minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        # read face from stream\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        rgb_face = rgb_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "            rgb_face = cv2.resize(rgb_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "        gray_face = preprocess_input(gray_face, False)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        rgb_face = preprocess_input(rgb_face, False)\n",
    "        rgb_face = np.expand_dims(rgb_face, 0)\n",
    "        rgb_face = np.expand_dims(rgb_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "\n",
    "        # style cv box\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "        \n",
    "        # re-recognize emotion every 10 iterations\n",
    "        if i % 10 == 0:\n",
    "            age_gender = model.predict(rgb_face.reshape((1, 64, 64, 3)))\n",
    "            predicted_genders = age_gender[0][0]\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            predicted_ages = int(age_gender[1].dot(ages).flatten()[0])\n",
    "            \n",
    "            if predicted_genders[0] > 0.5:\n",
    "                predicted_genders = 'male'\n",
    "            else:\n",
    "                predicted_genders = 'female'\n",
    "                \n",
    "            old_gender = predicted_genders\n",
    "            old_age = predicted_ages\n",
    "        else:\n",
    "            predicted_genders = old_gender\n",
    "            predicted_ages = old_age\n",
    "\n",
    "        # save recognized data\n",
    "        res_df.loc[i, :] = [time.time() - start_time, emotion_text, emotion_probability, predicted_genders, predicted_ages]\n",
    "        i += 1\n",
    "                   \n",
    "        # style cv box\n",
    "        emotion_window.append(emotion_text + ', ' + predicted_genders + ', ' + str(predicted_ages))\n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "        draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        draw_text(face_coordinates, rgb_image, emotion_mode,\n",
    "                  color, 0, -45, 1, 1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>reaction</th>\n",
       "      <th>reaction_tense</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.37727</td>\n",
       "      <td>sad</td>\n",
       "      <td>0.338094</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.88681</td>\n",
       "      <td>sad</td>\n",
       "      <td>0.363664</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.9451</td>\n",
       "      <td>sad</td>\n",
       "      <td>0.357571</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.9978</td>\n",
       "      <td>sad</td>\n",
       "      <td>0.388144</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.08983</td>\n",
       "      <td>sad</td>\n",
       "      <td>0.324722</td>\n",
       "      <td>male</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time reaction reaction_tense gender age\n",
       "0  1.37727      sad       0.338094   male  49\n",
       "1  2.88681      sad       0.363664   male  49\n",
       "2   2.9451      sad       0.357571   male  49\n",
       "3   2.9978      sad       0.388144   male  49\n",
       "4  3.08983      sad       0.324722   male  49"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res_df.to_csv(filename_result, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
